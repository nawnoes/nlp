{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NarrativeKoGPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0p6JShGUnpZ",
        "colab_type": "text"
      },
      "source": [
        "# NarrativeKoGPT2 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV7_Ye-1UuS2",
        "colab_type": "text"
      },
      "source": [
        "## 1.Google Drive 연동\n",
        "- 모델 파일과 학습 데이터가 저장 되어있는 구글 드라이브의 디렉토리와 Colab을 연동.  \n",
        "- 좌측상단 메뉴에서 런타임-> 런타임 유형 변경 -> 하드웨어 가속기 -> GPU 선택 후 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18LqQI0SVNX9",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 GPU 연동 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmKD5vgYUeTa",
        "colab_type": "code",
        "outputId": "0dfe324b-1a73-40fb-8d0b-76569e83489d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi2gIIroVXeS",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Google Drive 연동\n",
        "아래 코드를 실행후 나오는 URL을 클릭하여 나오는 인증 코드 입력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2tPgkJzUmBF",
        "colab_type": "code",
        "outputId": "a7dbf5ca-cc94-4dd1-81ce-bd07b592ed62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg07ZiFiVjJU",
        "colab_type": "code",
        "outputId": "ac548959-ef72-4d86-d606-7a686df7877d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS8lBXaKR6TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpCSwfFGkRx7",
        "colab_type": "text"
      },
      "source": [
        "**Colab 디렉토리 아래 NarrativeKoGPT2 경로 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7arJ4k2XLG_",
        "colab_type": "code",
        "outputId": "e7deb01d-1b11-4558-80fe-e67d8d8fc6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls drive/'My Drive'/'Colab Notebooks'/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT_X\tKorQuAD-beginner  NarrativeKoGPT2  NarrativeKoGPT2.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVmhNd21kse2",
        "colab_type": "text"
      },
      "source": [
        "**필요 패키지들 설치**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDrIL81uXPB0",
        "colab_type": "code",
        "outputId": "423f04bc-9b21-41c6-c2a3-5dd9b0619b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/NarrativeKoGPT2/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 3)) (0.1.85)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: transformers>=2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (2.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (20.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (0.29.15)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (1.18.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (1.12.18)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (7.1.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (1.15.18)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSCmVmaTlZ5S",
        "colab_type": "code",
        "outputId": "b5d5239d-e7b1-47bc-d886-5e16cc853514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "print(os.getcwd())\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hnSOCChk9lU",
        "colab_type": "text"
      },
      "source": [
        "## 2.KoGPT2 Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL6xVLtHn6vK",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.Import Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IGI-Rcakhsw",
        "colab_type": "code",
        "outputId": "49797ab0-c9c2-4cf4-f569-993a77ee771c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader # 데이터로더\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "from NarrativeKoGPT2.kogpt2.utils import get_tokenizer\n",
        "from NarrativeKoGPT2.kogpt2.utils import download, tokenizer\n",
        "from NarrativeKoGPT2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "from NarrativeKoGPT2.util.data import NovelDataset\n",
        "import gluonnlp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G20dHg4mn5x4",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. koGPT-2 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPoFzMKkk8eB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx= 'cpu'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
        "cachedir='~/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
        "epoch =200  # 학습 epoch\n",
        "\n",
        "pytorch_kogpt2 = {\n",
        "    'url':\n",
        "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'chksum': '676e9bcfa7'\n",
        "}\n",
        "kogpt2_config = {\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_epsilon\": 1e-05,\n",
        "    \"n_ctx\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_head\": 12,\n",
        "    \"n_layer\": 12,\n",
        "    \"n_positions\": 1024,\n",
        "    \"vocab_size\": 50000\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xIXykCtn45d",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Model and Vocab Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvtLJGh5o0MZ",
        "colab_type": "code",
        "outputId": "260614ca-1be8-4ab0-d586-5a312fc31fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# download model\n",
        "model_info = pytorch_kogpt2\n",
        "model_path = download(model_info['url'],\n",
        "                       model_info['fname'],\n",
        "                       model_info['chksum'],\n",
        "                       cachedir=cachedir)\n",
        "# download vocab\n",
        "vocab_info = tokenizer\n",
        "vocab_path = download(vocab_info['url'],\n",
        "                       vocab_info['fname'],\n",
        "                       vocab_info['chksum'],\n",
        "                       cachedir=cachedir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu7-2csBpLQR",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.KoGPT-2 Model Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5AK_S6spqwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
        "kogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "# model_path로부터 다운로드 받은 내용을 load_state_dict으로 업로드\n",
        "kogpt2model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "device = torch.device(ctx)\n",
        "kogpt2model.to(device)\n",
        "\n",
        "# kogpt2model.eval()\n",
        "# 추가로 학습하기 위해 .train() 사용\n",
        "kogpt2model.train()\n",
        "vocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "                                                     mask_token=None,\n",
        "                                                     sep_token=None,\n",
        "                                                     cls_token=None,\n",
        "                                                     unknown_token='<unk>',\n",
        "                                                     padding_token='<pad>',\n",
        "                                                     bos_token='<s>',\n",
        "                                                     eos_token='</s>')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rnV010Wq9Xw",
        "colab_type": "text"
      },
      "source": [
        "### 2.5. Get Batch Data using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukfj9FPHpwfk",
        "colab_type": "code",
        "outputId": "e004eaaf-25e6-4c63-f450-6fd84d6f6422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tok_path = get_tokenizer()\n",
        "model, vocab = kogpt2model, vocab_b_obj\n",
        "sentencepieceTokenizer = SentencepieceTokenizer(tok_path)\n",
        "\n",
        "#os.chdir(\"../\")\n",
        "data_file_path = 'drive/My Drive/Colab Notebooks/NarrativeKoGPT2/data/backmyo_novel_1/untokenized_bm_data.txt'\n",
        "batch_size = 4\n",
        "novel_dataset = NovelDataset(data_file_path, vocab,sentencepieceTokenizer)\n",
        "novel_data_loader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "(905,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RFndCOIrLS0",
        "colab_type": "text"
      },
      "source": [
        "### 2.6. Learning rate, Loss function, Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pY_o_C-qBhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-5\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgd28DRhthzo",
        "colab_type": "text"
      },
      "source": [
        "### 2.7. KoGPT-2 Transfer Laerning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYkDU-cbrduY",
        "colab_type": "code",
        "outputId": "a910779b-2de8-40b5-eb15-7c7875a45d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('KoGPT-2 Transfer Learning Start')\n",
        "epoch=200\n",
        "for epoch in range(epoch):\n",
        "  count = 0\n",
        "  for data in novel_data_loader:\n",
        "    print('train no.{}'.format(count+1))\n",
        "    optimizer.zero_grad()\n",
        "    # train_data = torch.tensor([data])\n",
        "    data = torch.stack(data) # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
        "\n",
        "    data= data.transpose(1,0)\n",
        "    # data= data.to(ctx)\n",
        "    # print(data.shape)\n",
        "\n",
        "    outputs = model(data, labels=data)\n",
        "    loss, logits = outputs[:2]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if count %10 ==0:\n",
        "      print('epoch no.{} train no.{}  loss = {}' . format(epoch, count+1, loss))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KoGPT-2 Transfer Learning Start\n",
            "train no.1\n",
            "epoch no.0 train no.1  loss = 4.408602237701416\n",
            "train no.2\n",
            "train no.3\n",
            "train no.4\n",
            "train no.5\n",
            "train no.6\n",
            "train no.7\n",
            "train no.8\n",
            "train no.9\n",
            "train no.10\n",
            "train no.11\n",
            "epoch no.0 train no.11  loss = 4.399487018585205\n",
            "train no.12\n",
            "train no.13\n",
            "train no.14\n",
            "train no.15\n",
            "train no.16\n",
            "train no.17\n",
            "train no.18\n",
            "train no.19\n",
            "train no.20\n",
            "train no.21\n",
            "epoch no.0 train no.21  loss = 4.43223762512207\n",
            "train no.22\n",
            "train no.23\n",
            "train no.24\n",
            "train no.25\n",
            "train no.26\n",
            "train no.27\n",
            "train no.28\n",
            "train no.29\n",
            "train no.30\n",
            "train no.31\n",
            "epoch no.0 train no.31  loss = 4.016036033630371\n",
            "train no.32\n",
            "train no.33\n",
            "train no.34\n",
            "train no.35\n",
            "train no.36\n",
            "train no.37\n",
            "train no.38\n",
            "train no.39\n",
            "train no.40\n",
            "train no.41\n",
            "epoch no.0 train no.41  loss = 4.198795795440674\n",
            "train no.42\n",
            "train no.43\n",
            "train no.44\n",
            "train no.45\n",
            "train no.46\n",
            "train no.47\n",
            "train no.48\n",
            "train no.49\n",
            "train no.50\n",
            "train no.51\n",
            "epoch no.0 train no.51  loss = 4.073155403137207\n",
            "train no.52\n",
            "train no.53\n",
            "train no.54\n",
            "train no.55\n",
            "train no.56\n",
            "train no.57\n",
            "train no.58\n",
            "train no.59\n",
            "train no.60\n",
            "train no.61\n",
            "epoch no.0 train no.61  loss = 4.060641288757324\n",
            "train no.62\n",
            "train no.63\n",
            "train no.64\n",
            "train no.65\n",
            "train no.66\n",
            "train no.67\n",
            "train no.68\n",
            "train no.69\n",
            "train no.70\n",
            "train no.71\n",
            "epoch no.0 train no.71  loss = 4.188094615936279\n",
            "train no.72\n",
            "train no.73\n",
            "train no.74\n",
            "train no.75\n",
            "train no.76\n",
            "train no.77\n",
            "train no.78\n",
            "train no.79\n",
            "train no.80\n",
            "train no.81\n",
            "epoch no.0 train no.81  loss = 3.9870223999023438\n",
            "train no.82\n",
            "train no.83\n",
            "train no.84\n",
            "train no.85\n",
            "train no.86\n",
            "train no.87\n",
            "train no.88\n",
            "train no.89\n",
            "train no.90\n",
            "train no.91\n",
            "epoch no.0 train no.91  loss = 4.216679096221924\n",
            "train no.92\n",
            "train no.93\n",
            "train no.94\n",
            "train no.95\n",
            "train no.96\n",
            "train no.97\n",
            "train no.98\n",
            "train no.99\n",
            "train no.100\n",
            "train no.101\n",
            "epoch no.0 train no.101  loss = 4.043731689453125\n",
            "train no.102\n",
            "train no.103\n",
            "train no.104\n",
            "train no.105\n",
            "train no.106\n",
            "train no.107\n",
            "train no.108\n",
            "train no.109\n",
            "train no.110\n",
            "train no.111\n",
            "epoch no.0 train no.111  loss = 4.121294021606445\n",
            "train no.112\n",
            "train no.113\n",
            "train no.114\n",
            "train no.115\n",
            "train no.116\n",
            "train no.117\n",
            "train no.118\n",
            "train no.119\n",
            "train no.120\n",
            "train no.121\n",
            "epoch no.0 train no.121  loss = 3.9655659198760986\n",
            "train no.122\n",
            "train no.123\n",
            "train no.124\n",
            "train no.125\n",
            "train no.126\n",
            "train no.127\n",
            "train no.128\n",
            "train no.129\n",
            "train no.130\n",
            "train no.131\n",
            "epoch no.0 train no.131  loss = 3.8244004249572754\n",
            "train no.132\n",
            "train no.133\n",
            "train no.134\n",
            "train no.135\n",
            "train no.136\n",
            "train no.137\n",
            "train no.138\n",
            "train no.139\n",
            "train no.140\n",
            "train no.141\n",
            "epoch no.0 train no.141  loss = 3.818635940551758\n",
            "train no.142\n",
            "train no.143\n",
            "train no.144\n",
            "train no.145\n",
            "train no.146\n",
            "train no.147\n",
            "train no.148\n",
            "train no.149\n",
            "train no.150\n",
            "train no.151\n",
            "epoch no.0 train no.151  loss = 3.9016435146331787\n",
            "train no.152\n",
            "train no.153\n",
            "train no.154\n",
            "train no.155\n",
            "train no.156\n",
            "train no.157\n",
            "train no.158\n",
            "train no.159\n",
            "train no.160\n",
            "train no.161\n",
            "epoch no.0 train no.161  loss = 3.808495044708252\n",
            "train no.162\n",
            "train no.163\n",
            "train no.164\n",
            "train no.165\n",
            "train no.166\n",
            "train no.167\n",
            "train no.168\n",
            "train no.169\n",
            "train no.170\n",
            "train no.171\n",
            "epoch no.0 train no.171  loss = 4.082968711853027\n",
            "train no.172\n",
            "train no.173\n",
            "train no.174\n",
            "train no.175\n",
            "train no.176\n",
            "train no.177\n",
            "train no.178\n",
            "train no.179\n",
            "train no.180\n",
            "train no.181\n",
            "epoch no.0 train no.181  loss = 4.155140399932861\n",
            "train no.182\n",
            "train no.183\n",
            "train no.184\n",
            "train no.185\n",
            "train no.186\n",
            "train no.187\n",
            "train no.188\n",
            "train no.189\n",
            "train no.190\n",
            "train no.191\n",
            "epoch no.0 train no.191  loss = 4.0360846519470215\n",
            "train no.192\n",
            "train no.193\n",
            "train no.194\n",
            "train no.195\n",
            "train no.196\n",
            "train no.197\n",
            "train no.198\n",
            "train no.199\n",
            "train no.200\n",
            "train no.201\n",
            "epoch no.0 train no.201  loss = 3.7935492992401123\n",
            "train no.202\n",
            "train no.203\n",
            "train no.204\n",
            "train no.205\n",
            "train no.206\n",
            "train no.207\n",
            "train no.208\n",
            "train no.209\n",
            "train no.210\n",
            "train no.211\n",
            "epoch no.0 train no.211  loss = 3.9370853900909424\n",
            "train no.212\n",
            "train no.213\n",
            "train no.214\n",
            "train no.215\n",
            "train no.216\n",
            "train no.217\n",
            "train no.218\n",
            "train no.219\n",
            "train no.220\n",
            "train no.221\n",
            "epoch no.0 train no.221  loss = 4.061526298522949\n",
            "train no.222\n",
            "train no.223\n",
            "train no.224\n",
            "train no.225\n",
            "train no.226\n",
            "train no.227\n",
            "train no.1\n",
            "epoch no.1 train no.1  loss = 3.849163293838501\n",
            "train no.2\n",
            "train no.3\n",
            "train no.4\n",
            "train no.5\n",
            "train no.6\n",
            "train no.7\n",
            "train no.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by1zYlUWudTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}