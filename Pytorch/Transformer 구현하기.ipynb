{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 구현하기\n",
    "현재 가장 좋은 성능을 내는 모델이 transformer 기반의 Pretrained 모델. [Attention is All You Need](https://arxiv.org/abs/1706.03762) 논문 참조 할수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocab\n",
    "SentencePiece를 활용해 만든 vocab을 이용해 텍스트를 tensor로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab_file = \"<path of data>/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)\n",
    "\n",
    "# 입력 texts\n",
    "lines = [\n",
    "  \"겨울은 추워요.\",\n",
    "  \"감기 조심하세요.\"\n",
    "]\n",
    "\n",
    "# text를 tensor로 변환\n",
    "inputs = []\n",
    "for line in lines:\n",
    "  pieces = vocab.encode_as_pieces(line)\n",
    "  ids = vocab.encode_as_ids(line)\n",
    "  inputs.append(torch.tensor(ids))\n",
    "  print(pieces)\n",
    "\n",
    "# 입력 길이가 다르므로 입력 최대 길이에 맟춰 padding(0)을 추가 해 줌\n",
    "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "# shape\n",
    "print(inputs.size())\n",
    "# 값\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행결과\n",
    "```\n",
    "['▁겨울', '은', '▁추', '워', '요', '.']\n",
    "['▁감', '기', '▁조', '심', '하', '세', '요', '.']\n",
    "torch.Size([2, 8])\n",
    "tensor([[3091, 3604,  206, 3958, 3760, 3590,    0,    0],\n",
    "        [ 212, 3605,   53, 3832, 3596, 3682, 3760, 3590]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding\n",
    "Transformer의 Embedding은 **Input Embedding**과 **Position Embedding** 두가지를 합해서 사용.  \n",
    "### 2.1 Input Embedding  \n",
    "Embedding은 입력 토큰을 vector로 형태로 변환\n",
    "    1) innputs에 대한 embedding 값 input_embs를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab) # vocab count\n",
    "d_hidn = 128 # hidden size\n",
    "nn_emb = nn.Embedding(n_vocab, d_hidn) # embedding 객체\n",
    "\n",
    "input_embs = nn_emb(inputs) # input embedding\n",
    "print(input_embs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 input(2,8)에 대한 Embedding 값 input_embs(2,8,128) shape을 갖는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([2, 8, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Position Embedding  \n",
    "position **encoding** 값을 구하기 위한 과정\n",
    "    - 각 position 별로 angle 값을 구한다\n",
    "    - 구해진 angle 중 짝수 index의 값에 대한 sin 값을 구한다.\n",
    "    - 구해진 angle 중 홀수 index의 값에 대한 cos 값을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sinusoid position embedding \"\"\"\n",
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Position Encoding 구하는 방법**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 64\n",
    "pos_encoding = get_sinusoid_encoding_table(n_seq, d_hidn)\n",
    "\n",
    "print (pos_encoding.shape) # 크기 출력\n",
    "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, d_hidn))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Position Encoding 값으로 Position Embedding 생성**  \n",
    "    - 구해진 Position encoding 값을 이용해 position embedding을 생성. 학습되는 값이 아니므로 freeze 옵션은 True로 설정\n",
    "    - 입력 inputs과 동일한 크기를 갖는 position 값을 구한다.\n",
    "    - input 값 중 pad(0)값을 찾는다.\n",
    "    - position 값중 pad 부분은 0으로 변경\n",
    "    - position 값에 해당하는 embedding 값을 구한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = torch.FloatTensor(pos_encoding)\n",
    "nn_pos = nn.Embedding.from_pretrained(pos_encoding, freeze=True)\n",
    "\n",
    "positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "pos_mask = inputs.eq(0)\n",
    "\n",
    "positions.masked_fill_(pos_mask, 0)\n",
    "pos_embs = nn_pos(positions) # position embedding\n",
    "\n",
    "print(inputs)\n",
    "print(positions)\n",
    "print(pos_embs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 구한 **input_embs**와 **pos_embs**를 더하면 transformer에 입력할 input이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sums = input_embs + pos_embsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot Product Attention\n",
    "![](https://paul-hyun.github.io/assets/2019-12-19/scale_dot_product_attention.png)\n",
    "\n",
    "### 3.1 입력값\n",
    "입력값은 Q(query), K(key), V(Value) 그리고 attention mask로 구성되어 있다. 입력 값중 K,V는 같아야하고, Q,K,V가 모두 동일한 경우 self attention이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = input_sums\n",
    "K = input_sums\n",
    "V = input_sums\n",
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "print(attn_mask.size())\n",
    "print(attn_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MatMul Q, K-transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "print(scores.size())\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_head = 64\n",
    "scores = scores.mul_(1/d_head**0.5)\n",
    "print(scores.size())\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Mask(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.masked_fill_(attn_mask, -1e9)\n",
    "print(scores.size())\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "print(attn_prob.size())\n",
    "print(attn_prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Matmul(attn_prob, V)\n",
    "위 수식 중 5번 attention_probablity * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.matmul(attn_prob, V)\n",
    "print(context.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 클래스로 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" scale dot product attention \"\"\"\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        # (bs, n_head, n_q_seq, d_v)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Multi Head Attention\n",
    "![](https://paul-hyun.github.io/assets/2019-12-19/multi_head_attention.png)\n",
    "\n",
    "### 입력값\n",
    "Q, K, V, attn_mask는 ScaledDotProductAttention과 동일하고, head의 갯수는 2개, head의 dimension은 64이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = input_sums\n",
    "K = input_sums\n",
    "V = input_sums\n",
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "\n",
    "batch_size = Q.size(0)\n",
    "n_head = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Q, K, V\n",
    "위 그림 수식 중 1번 Q를 여러개의 head로 나누는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_K = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_V = nn.Linear(d_hidn, n_head * d_head)\n",
    "\n",
    "# (bs, n_seq, n_head * d_head)\n",
    "q_s = W_Q(Q)\n",
    "print(q_s.size())\n",
    "# (bs, n_seq, n_head, d_head)\n",
    "q_s = q_s.view(batch_size, -1, n_head, d_head)\n",
    "print(q_s.size())\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "q_s = q_s.transpose(1,2)\n",
    "print(q_s.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q값이 head 단위로 나누어 졌다\n",
    "```\n",
    "torch.Size([2, 8, 128])\n",
    "torch.Size([2, 8, 2, 64])\n",
    "torch.Size([2, 2, 8, 64])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 과정을 한줄로 표현하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W_Q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e3819f34bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (bs, n_head, n_seq, d_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# (bs, n_head, n_seq, d_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_K\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# (bs, n_head, n_seq, d_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'W_Q' is not defined"
     ]
    }
   ],
   "source": [
    "# (bs, n_head, n_seq, d_head)\n",
    "q_s = W_Q(Q).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "k_s = W_K(K).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "v_s = W_V(V).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "print(q_s.size(), k_s.size(), v_s.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Mask도 Multi Head로 변경\n",
    "```\n",
    "torch.Size([2, 8, 8])\n",
    "torch.Size([2, 2, 8, 8])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "위 그림 수식 중 4번 Attenstion 과정으로, 앞 부분의 Scaled Dot Product Attention을 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dot_attn = ScaledDotProductAttention(d_head)\n",
    "context, attn_prob = scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "print(context.size())\n",
    "print(attn_prob.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 과정에서 multi head에 대한 attention을 구했다.\n",
    "```\n",
    "torch.Size([2, 2, 8, 64])\n",
    "torch.Size([2, 2, 8, 8])\n",
    "```\n",
    "  \n",
    "### Concat\n",
    "위 그림 수식 중 5번 Concat 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (bs, n_seq, n_head * d_head)\n",
    "context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_head * d_head)\n",
    "print(context.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concat을 통해 multi head를 한개로 합쳤다.\n",
    "```\n",
    "torch.Size([2, 8, 128])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear\n",
    "위 그림 수식 중 6번 Linear 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(n_head * d_head, d_hidn)\n",
    "# (bs, n_seq, d_hidn)\n",
    "output = linear(context)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 Q와 동일한 Shape을 가진 Multi Head Attention이 구해졌다.\n",
    "```\n",
    "torch.Size([2, 8, 128])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class\n",
    "위 절차를 하나의 클래스 형태로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" multi head attention \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_hidn, n_head, d_head):\n",
    "        super().__init__()\n",
    "        self.d_hidn = d_hidn\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "\n",
    "        self.W_Q = nn.Linear(d_hidn, n_head * d_head)\n",
    "        self.W_K = nn.Linear(d_hidn, n_head * d_head)\n",
    "        self.W_V = nn.Linear(d_hidn, n_head * d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(d_head)\n",
    "        self.linear = nn.Linear(n_head * d_head, d_hidn)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Masked Multi-Head Attention\n",
    "Masked Multi-Head Attention은 Multi Head Attention과 attention mask를 제외한 부분은 모두 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention decoder mask \"\"\"\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "Q = input_sums\n",
    "K = input_sums\n",
    "V = input_sums\n",
    "\n",
    "attn_pad_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "print(attn_pad_mask[1])\n",
    "attn_dec_mask = get_attn_decoder_mask(inputs)\n",
    "print(attn_dec_mask[1])\n",
    "attn_mask = torch.gt((attn_pad_mask + attn_dec_mask), 0)\n",
    "print(attn_mask[1])\n",
    "\n",
    "batch_size = Q.size(0)\n",
    "n_head = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad mask, decoder mask 그리고 이 둘을 합한 attention mask를 확인 할 수 있다.\n",
    "```\n",
    "tensor([[False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True]])\n",
    "tensor([[0, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [0, 0, 1, 1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
    "        [False, False,  True,  True,  True,  True,  True,  True],\n",
    "        [False, False, False,  True,  True,  True,  True,  True],\n",
    "        [False, False, False, False,  True,  True,  True,  True],\n",
    "        [False, False, False, False, False,  True,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True],\n",
    "        [False, False, False, False, False, False,  True,  True]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "Multi-head attention과 동일하므로, 위에서 선언한 MultiHeadAttention클래스를 바로 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiHeadAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0fdf7eeb682b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_hidn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiHeadAttention' is not defined"
     ]
    }
   ],
   "source": [
    "attention = MultiHeadAttention(d_hidn, n_head, d_head)\n",
    "output, attn_prob = attention(Q, K, V, attn_mask)\n",
    "print(output.size(), attn_prob.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FeedForwar\n",
    "![](https://paul-hyun.github.io/assets/2019-12-19/feed-forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1(Linear)\n",
    "위 그림 수식 중 1번 f1(Linear) 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(in_channels=d_hidn, out_channels=d_hidn * 4, kernel_size=1)\n",
    "# (bs, d_hidn * 4, n_seq)\n",
    "ff_1 = conv1(output.transpose(1, 2))\n",
    "print(ff_1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력에 비해 hidden dimension이 4배 커졌다. out_channels = d_hidn*4 으로 출력이 커졌다.\n",
    "```\n",
    "torch.Size([2, 512, 8])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation(relu or gelu)\n",
    "위 그림 수식 중 2번 Activation(relu or gelu)과정. 논문이 발표될 당시 relu를 사용했지만 이후 gelu가 성능에 더 좋다는 것이 발견\n",
    "![](https://paul-hyun.github.io/assets/2019-12-19/activation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active = F.relu\n",
    "active = F.gelu\n",
    "ff_2 = active(ff_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f3(Linear)\n",
    "위 그림 수식 중 3번 f3(Linear) 과정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = nn.Conv1d(in_channels=d_hidn * 4, out_channels=d_hidn, kernel_size=1)\n",
    "ff_3 = conv2(ff_2).transpose(1, 2)\n",
    "print(ff_3.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력과 동일한 shape으로 변경된 결과를 확인\n",
    "```\n",
    "torch.Size([2, 8, 128])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class \n",
    "위 절차를 하나의 클래스 형태로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" feed forward \"\"\"\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_hidn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_hidn * 4, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_hidn * 4, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
